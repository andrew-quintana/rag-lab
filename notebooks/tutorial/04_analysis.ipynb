{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Results Analysis and Visualization\n",
    "\n",
    "This notebook provides comprehensive analysis and visualization of evaluation results.\n",
    "\n",
    "## Objectives\n",
    "- Load and compare multiple evaluation runs\n",
    "- Perform deep analysis of performance patterns\n",
    "- Generate publication-quality visualizations\n",
    "- Export insights and recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport sys\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport logging\nfrom datetime import datetime\nimport json\nfrom collections import defaultdict\n\n# Add src to path\nsys.path.append('../src')\n\n# Import raglab modules\nfrom core.io import RunManager\nfrom eval import load_evaluation_results\n\n# Set up logging and plotting\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\nprint(\"âœ… Imports successful\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover and Load Evaluation Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover available evaluation runs\n",
    "run_manager = RunManager(runs_base_path='../runs')\n",
    "available_runs = run_manager.list_runs()\n",
    "\n",
    "print(f\"ğŸ” Found {len(available_runs)} evaluation runs:\")\n",
    "for i, run_name in enumerate(available_runs):\n",
    "    print(f\"   {i+1}. {run_name}\")\n",
    "\n",
    "if not available_runs:\n",
    "    print(\"âš ï¸  No evaluation runs found. Please run notebooks 02 or 03 first.\")\n",
    "else:\n",
    "    print(f\"\\nğŸ“Š Analysis will focus on these runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all evaluation runs\n",
    "run_data = {}\n",
    "run_base_path = Path('../runs')\n",
    "\n",
    "for run_name in available_runs:\n",
    "    run_path = run_base_path / run_name\n",
    "    \n",
    "    try:\n",
    "        # Load config, outputs, and metrics\n",
    "        config = run_manager.load_config(str(run_path))\n",
    "        outputs, metrics = load_evaluation_results(str(run_path))\n",
    "        \n",
    "        run_data[run_name] = {\n",
    "            'config': config,\n",
    "            'outputs': outputs,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… Loaded {run_name}: {len(outputs)} examples\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load {run_name}: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ“š Successfully loaded {len(run_data)} evaluation runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Comparison Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison summary\n",
    "if len(run_data) > 1:\n",
    "    print(\"ğŸ“Š Run Comparison Summary:\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for run_name, data in run_data.items():\n",
    "        metrics = data['metrics']\n",
    "        config = data['config']\n",
    "        \n",
    "        row = {\n",
    "            'run_name': run_name,\n",
    "            'type': config.get('evaluation_type', 'unknown'),\n",
    "            'examples': metrics.get('total_examples', 0),\n",
    "            'error_rate': metrics.get('error_rate', 0)\n",
    "        }\n",
    "        \n",
    "        # Add BEIR metrics if available\n",
    "        if 'beir_metrics' in metrics:\n",
    "            beir = metrics['beir_metrics']\n",
    "            row.update({\n",
    "                'avg_recall': beir.get('recall_at_k', {}).get('mean', 0),\n",
    "                'avg_precision': beir.get('precision_at_k', {}).get('mean', 0),\n",
    "                'avg_ndcg': beir.get('ndcg_at_k', {}).get('mean', 0)\n",
    "            })\n",
    "        \n",
    "        # Add judge metrics if available\n",
    "        if 'judge_metrics' in metrics:\n",
    "            judge = metrics['judge_metrics']\n",
    "            row.update({\n",
    "                'correctness_rate': judge.get('correctness_rate', 0),\n",
    "                'hallucination_rate': judge.get('hallucination_rate', 0)\n",
    "            })\n",
    "        \n",
    "        # Add meta-eval metrics if available\n",
    "        if 'meta_eval_metrics' in metrics:\n",
    "            meta = metrics['meta_eval_metrics']\n",
    "            row['judge_accuracy'] = meta.get('judge_accuracy', 0)\n",
    "        \n",
    "        comparison_data.append(row)\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "else:\n",
    "    print(\"ğŸ“Š Single run analysis (comparison requires multiple runs)\")\n",
    "    comparison_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance trends across runs\n",
    "if comparison_df is not None and len(comparison_df) > 1:\n",
    "    \n",
    "    # Sort runs by timestamp (extract from run name)\n",
    "    comparison_df['timestamp'] = comparison_df['run_name'].str.extract(r'(\\d{4}-\\d{2}-\\d{2}_\\d{4})')\n",
    "    comparison_df = comparison_df.sort_values('timestamp')\n",
    "    \n",
    "    # Create performance trend visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. BEIR Metrics Trend\n",
    "    if 'avg_recall' in comparison_df.columns:\n",
    "        axes[0,0].plot(range(len(comparison_df)), comparison_df['avg_recall'], 'o-', label='Recall@K', linewidth=2)\n",
    "        axes[0,0].plot(range(len(comparison_df)), comparison_df['avg_precision'], 's-', label='Precision@K', linewidth=2)\n",
    "        axes[0,0].plot(range(len(comparison_df)), comparison_df['avg_ndcg'], '^-', label='nDCG@K', linewidth=2)\n",
    "        axes[0,0].set_title('BEIR Metrics Trend')\n",
    "        axes[0,0].set_xlabel('Run Number')\n",
    "        axes[0,0].set_ylabel('Score')\n",
    "        axes[0,0].legend()\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "        axes[0,0].set_xticks(range(len(comparison_df)))\n",
    "        axes[0,0].set_xticklabels([f\"Run {i+1}\" for i in range(len(comparison_df))])\n",
    "    else:\n",
    "        axes[0,0].text(0.5, 0.5, 'No BEIR Metrics\\nAvailable', ha='center', va='center', transform=axes[0,0].transAxes)\n",
    "    \n",
    "    # 2. Judge Performance Trend\n",
    "    if 'correctness_rate' in comparison_df.columns:\n",
    "        axes[0,1].plot(range(len(comparison_df)), comparison_df['correctness_rate'], 'o-', label='Correctness Rate', linewidth=2, color='green')\n",
    "        axes[0,1].plot(range(len(comparison_df)), 1-comparison_df['hallucination_rate'], 's-', label='No Hallucination Rate', linewidth=2, color='blue')\n",
    "        if 'judge_accuracy' in comparison_df.columns:\n",
    "            axes[0,1].plot(range(len(comparison_df)), comparison_df['judge_accuracy'], '^-', label='Judge Accuracy', linewidth=2, color='purple')\n",
    "        axes[0,1].set_title('Judge Performance Trend')\n",
    "        axes[0,1].set_xlabel('Run Number')\n",
    "        axes[0,1].set_ylabel('Rate')\n",
    "        axes[0,1].legend()\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "        axes[0,1].set_xticks(range(len(comparison_df)))\n",
    "        axes[0,1].set_xticklabels([f\"Run {i+1}\" for i in range(len(comparison_df))])\n",
    "    else:\n",
    "        axes[0,1].text(0.5, 0.5, 'No Judge Metrics\\nAvailable', ha='center', va='center', transform=axes[0,1].transAxes)\n",
    "    \n",
    "    # 3. Error Rate Trend\n",
    "    axes[1,0].plot(range(len(comparison_df)), comparison_df['error_rate'], 'o-', linewidth=2, color='red')\n",
    "    axes[1,0].set_title('Error Rate Trend')\n",
    "    axes[1,0].set_xlabel('Run Number')\n",
    "    axes[1,0].set_ylabel('Error Rate')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    axes[1,0].set_xticks(range(len(comparison_df)))\n",
    "    axes[1,0].set_xticklabels([f\"Run {i+1}\" for i in range(len(comparison_df))])\n",
    "    \n",
    "    # 4. Run Summary Bar Chart\n",
    "    metric_cols = [col for col in ['avg_recall', 'correctness_rate', 'judge_accuracy'] if col in comparison_df.columns]\n",
    "    if metric_cols:\n",
    "        x = np.arange(len(comparison_df))\n",
    "        width = 0.2\n",
    "        \n",
    "        for i, col in enumerate(metric_cols):\n",
    "            axes[1,1].bar(x + i*width, comparison_df[col], width, label=col.replace('_', ' ').title())\n",
    "        \n",
    "        axes[1,1].set_title('Performance Summary')\n",
    "        axes[1,1].set_xlabel('Run Number')\n",
    "        axes[1,1].set_ylabel('Score')\n",
    "        axes[1,1].set_xticks(x + width)\n",
    "        axes[1,1].set_xticklabels([f\"Run {i+1}\" for i in range(len(comparison_df))])\n",
    "        axes[1,1].legend()\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Evaluation Performance Trends', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  Trend analysis requires multiple runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Dive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the most recent or best performing run for deep analysis\n",
    "if run_data:\n",
    "    # Choose the most recent run (or you can modify this logic)\n",
    "    selected_run = max(run_data.keys())\n",
    "    selected_data = run_data[selected_run]\n",
    "    \n",
    "    print(f\"ğŸ” Deep Analysis of: {selected_run}\")\n",
    "    print(f\"   Type: {selected_data['config'].get('evaluation_type', 'unknown')}\")\n",
    "    print(f\"   Examples: {len(selected_data['outputs'])}\")\n",
    "    \n",
    "    # Convert outputs to DataFrame for detailed analysis\n",
    "    detailed_df = pd.DataFrame(selected_data['outputs'])\n",
    "    \n",
    "    # Extract nested data if this is a complete agent evaluation\n",
    "    if 'judge_output' in detailed_df.columns:\n",
    "        # Extract judge metrics\n",
    "        judge_metrics = pd.json_normalize(detailed_df['judge_output'])\n",
    "        meta_metrics = pd.json_normalize(detailed_df['meta_eval_output'])\n",
    "        beir_metrics = pd.json_normalize(detailed_df['beir_metrics'])\n",
    "        \n",
    "        # Combine into analysis DataFrame\n",
    "        analysis_df = pd.concat([\n",
    "            detailed_df[['example_id']],\n",
    "            judge_metrics.add_prefix('judge_'),\n",
    "            meta_metrics.add_prefix('meta_'),\n",
    "            beir_metrics.add_prefix('beir_')\n",
    "        ], axis=1)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Analysis DataFrame created with {len(analysis_df)} examples and {len(analysis_df.columns)} metrics\")\n",
    "        \n",
    "    else:\n",
    "        analysis_df = detailed_df\n",
    "        print(f\"\\nğŸ“Š Using raw outputs for analysis\")\n",
    "else:\n",
    "    print(\"âŒ No runs available for deep analysis\")\n",
    "    analysis_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance pattern analysis\n",
    "if analysis_df is not None and 'judge_correctness_binary' in analysis_df.columns:\n",
    "    \n",
    "    print(\"ğŸ¯ Performance Pattern Analysis:\")\n",
    "    \n",
    "    # Correlation analysis\n",
    "    numeric_cols = analysis_df.select_dtypes(include=[np.number]).columns\n",
    "    correlation_matrix = analysis_df[numeric_cols].corr()\n",
    "    \n",
    "    # Create correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": .5}, fmt='.2f')\n",
    "    plt.title('Performance Metrics Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify strong correlations\n",
    "    strong_correlations = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr) > 0.5:  # Strong correlation threshold\n",
    "                col1 = correlation_matrix.columns[i]\n",
    "                col2 = correlation_matrix.columns[j]\n",
    "                strong_correlations.append((col1, col2, corr))\n",
    "    \n",
    "    if strong_correlations:\n",
    "        print(f\"\\nğŸ”— Strong Correlations (|r| > 0.5):\")\n",
    "        for col1, col2, corr in sorted(strong_correlations, key=lambda x: abs(x[2]), reverse=True):\n",
    "            print(f\"   {col1} â†” {col2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(f\"\\nğŸ“Š No strong correlations found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze failure patterns\n",
    "if analysis_df is not None and 'judge_correctness_binary' in analysis_df.columns:\n",
    "    \n",
    "    print(\"âŒ Failure Analysis:\")\n",
    "    \n",
    "    # Identify failed examples\n",
    "    failed_examples = analysis_df[~analysis_df['judge_correctness_binary']]\n",
    "    successful_examples = analysis_df[analysis_df['judge_correctness_binary']]\n",
    "    \n",
    "    print(f\"   Failed examples: {len(failed_examples)}\")\n",
    "    print(f\"   Successful examples: {len(successful_examples)}\")\n",
    "    \n",
    "    if len(failed_examples) > 0:\n",
    "        # Compare failed vs successful examples\n",
    "        print(f\"\\nğŸ“Š Failed vs Successful Comparison:\")\n",
    "        \n",
    "        comparison_metrics = ['beir_recall_at_k', 'beir_precision_at_k', 'beir_ndcg_at_k']\n",
    "        available_metrics = [m for m in comparison_metrics if m in analysis_df.columns]\n",
    "        \n",
    "        if available_metrics:\n",
    "            fig, axes = plt.subplots(1, len(available_metrics), figsize=(5*len(available_metrics), 5))\n",
    "            if len(available_metrics) == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            for i, metric in enumerate(available_metrics):\n",
    "                # Box plot comparison\n",
    "                data_to_plot = [\n",
    "                    failed_examples[metric].dropna(),\n",
    "                    successful_examples[metric].dropna()\n",
    "                ]\n",
    "                labels = ['Failed', 'Successful']\n",
    "                \n",
    "                box_plot = axes[i].boxplot(data_to_plot, labels=labels, patch_artist=True)\n",
    "                axes[i].set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "                axes[i].set_ylabel('Score')\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Color boxes\n",
    "                colors = ['lightcoral', 'lightgreen']\n",
    "                for patch, color in zip(box_plot['boxes'], colors):\n",
    "                    patch.set_facecolor(color)\n",
    "                \n",
    "                # Add statistical comparison\n",
    "                failed_mean = failed_examples[metric].mean()\n",
    "                success_mean = successful_examples[metric].mean()\n",
    "                axes[i].text(0.5, 0.95, f'Failed: {failed_mean:.3f}\\nSuccess: {success_mean:.3f}', \n",
    "                           transform=axes[i].transAxes, ha='center', va='top', \n",
    "                           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.suptitle('Failed vs Successful Examples Comparison', fontsize=14, fontweight='bold', y=1.02)\n",
    "            plt.show()\n",
    "            \n",
    "        # Show specific failed examples\n",
    "        print(f\"\\nğŸ” Failed Examples Details:\")\n",
    "        for _, row in failed_examples.iterrows():\n",
    "            print(f\"   {row['example_id']}:\")\n",
    "            if 'beir_recall_at_k' in row:\n",
    "                print(f\"     Retrieval: R={row['beir_recall_at_k']:.3f}, P={row['beir_precision_at_k']:.3f}\")\n",
    "            if 'judge_hallucination_binary' in row:\n",
    "                print(f\"     Hallucination: {row['judge_hallucination_binary']}\")\n",
    "            if 'meta_judge_correct' in row:\n",
    "                print(f\"     Judge Reliable: {row['meta_judge_correct']}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"âœ… No failed examples found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Analysis (Insurance Specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze insurance risk patterns\n",
    "if analysis_df is not None and 'judge_risk_direction' in analysis_df.columns:\n",
    "    \n",
    "    risk_data = analysis_df['judge_risk_direction'].dropna()\n",
    "    impact_data = analysis_df['judge_risk_impact'].dropna()\n",
    "    \n",
    "    if len(risk_data) > 0:\n",
    "        print(\"ğŸ¥ Insurance Risk Analysis:\")\n",
    "        \n",
    "        # Risk direction distribution\n",
    "        risk_counts = risk_data.value_counts().sort_index()\n",
    "        risk_labels = {\n",
    "            -1: 'Care Avoidance Risk\\n(Overestimated Cost)',\n",
    "            0: 'No Clear Risk Direction',\n",
    "            1: 'Unexpected Cost Risk\\n(Underestimated Cost)'\n",
    "        }\n",
    "        \n",
    "        # Create risk analysis visualization\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Risk direction pie chart\n",
    "        labels = [risk_labels.get(idx, f'Risk {idx}') for idx in risk_counts.index]\n",
    "        colors = ['lightcoral', 'lightgray', 'orange']\n",
    "        risk_colors = [colors[idx+1] for idx in risk_counts.index]  # Map -1,0,1 to 0,1,2\n",
    "        \n",
    "        axes[0].pie(risk_counts.values, labels=labels, autopct='%1.1f%%', colors=risk_colors)\n",
    "        axes[0].set_title('Risk Direction Distribution', fontweight='bold')\n",
    "        \n",
    "        # Risk impact distribution\n",
    "        if len(impact_data) > 0:\n",
    "            impact_counts = impact_data.value_counts().sort_index()\n",
    "            impact_labels = ['Minimal (0)', 'Low (1)', 'Moderate (2)', 'High (3)']\n",
    "            \n",
    "            bars = axes[1].bar(impact_counts.index, impact_counts.values, \n",
    "                             color=['lightgreen', 'yellow', 'orange', 'red'])\n",
    "            axes[1].set_title('Risk Impact Distribution', fontweight='bold')\n",
    "            axes[1].set_xlabel('Impact Level')\n",
    "            axes[1].set_ylabel('Count')\n",
    "            axes[1].set_xticks(impact_counts.index)\n",
    "            axes[1].set_xticklabels([impact_labels[i] for i in impact_counts.index])\n",
    "            \n",
    "            # Add count labels on bars\n",
    "            for bar, count in zip(bars, impact_counts.values):\n",
    "                axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                           str(count), ha='center', va='bottom')\n",
    "        else:\n",
    "            axes[1].text(0.5, 0.5, 'No Risk Impact\\nData Available', \n",
    "                        ha='center', va='center', transform=axes[1].transAxes)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Risk statistics\n",
    "        print(f\"\\nğŸ“Š Risk Statistics:\")\n",
    "        total_risk_cases = len(risk_data)\n",
    "        care_avoidance = sum(risk_data == -1)\n",
    "        unexpected_cost = sum(risk_data == 1)\n",
    "        no_clear_direction = sum(risk_data == 0)\n",
    "        \n",
    "        print(f\"   Total risk cases: {total_risk_cases}\")\n",
    "        print(f\"   Care avoidance risk: {care_avoidance} ({care_avoidance/total_risk_cases:.1%})\")\n",
    "        print(f\"   Unexpected cost risk: {unexpected_cost} ({unexpected_cost/total_risk_cases:.1%})\")\n",
    "        print(f\"   No clear direction: {no_clear_direction} ({no_clear_direction/total_risk_cases:.1%})\")\n",
    "        \n",
    "        if len(impact_data) > 0:\n",
    "            avg_impact = impact_data.mean()\n",
    "            high_impact = sum(impact_data >= 2)\n",
    "            print(f\"   Average impact level: {avg_impact:.2f}\")\n",
    "            print(f\"   High impact cases (â‰¥2): {high_impact} ({high_impact/len(impact_data):.1%})\")\n",
    "            \n",
    "    else:\n",
    "        print(\"ğŸ“Š No insurance risk data available in this evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive insights report\n",
    "insights_report = {\n",
    "    'analysis_timestamp': datetime.now().isoformat(),\n",
    "    'runs_analyzed': len(run_data),\n",
    "    'insights': [],\n",
    "    'recommendations': []\n",
    "}\n",
    "\n",
    "if run_data:\n",
    "    # Overall insights\n",
    "    if comparison_df is not None and len(comparison_df) > 1:\n",
    "        # Multi-run insights\n",
    "        best_run = comparison_df.loc[comparison_df['avg_recall'].idxmax()] if 'avg_recall' in comparison_df.columns else None\n",
    "        if best_run is not None:\n",
    "            insights_report['insights'].append(f\"Best performing run: {best_run['run_name']} with {best_run['avg_recall']:.3f} recall@k\")\n",
    "        \n",
    "        # Performance trends\n",
    "        if 'avg_recall' in comparison_df.columns:\n",
    "            recall_trend = comparison_df['avg_recall'].diff().mean()\n",
    "            if recall_trend > 0.01:\n",
    "                insights_report['insights'].append(\"Positive retrieval performance trend observed across runs\")\n",
    "            elif recall_trend < -0.01:\n",
    "                insights_report['insights'].append(\"Declining retrieval performance trend - investigate causes\")\n",
    "    \n",
    "    # Single run insights\n",
    "    if analysis_df is not None:\n",
    "        # Performance insights\n",
    "        if 'judge_correctness_binary' in analysis_df.columns:\n",
    "            correctness_rate = analysis_df['judge_correctness_binary'].mean()\n",
    "            insights_report['insights'].append(f\"Overall correctness rate: {correctness_rate:.1%}\")\n",
    "            \n",
    "            if correctness_rate < 0.7:\n",
    "                insights_report['recommendations'].append(\"Improve generation quality - consider better prompts or models\")\n",
    "        \n",
    "        if 'beir_recall_at_k' in analysis_df.columns:\n",
    "            avg_recall = analysis_df['beir_recall_at_k'].mean()\n",
    "            insights_report['insights'].append(f\"Average retrieval recall: {avg_recall:.3f}\")\n",
    "            \n",
    "            if avg_recall < 0.7:\n",
    "                insights_report['recommendations'].append(\"Improve retrieval system - consider different embeddings or chunking strategies\")\n",
    "        \n",
    "        # Risk insights\n",
    "        if 'judge_risk_direction' in analysis_df.columns:\n",
    "            risk_data = analysis_df['judge_risk_direction'].dropna()\n",
    "            if len(risk_data) > 0:\n",
    "                care_avoidance_rate = sum(risk_data == -1) / len(risk_data)\n",
    "                unexpected_cost_rate = sum(risk_data == 1) / len(risk_data)\n",
    "                \n",
    "                insights_report['insights'].append(f\"Care avoidance risk rate: {care_avoidance_rate:.1%}\")\n",
    "                insights_report['insights'].append(f\"Unexpected cost risk rate: {unexpected_cost_rate:.1%}\")\n",
    "                \n",
    "                if care_avoidance_rate > 0.3:\n",
    "                    insights_report['recommendations'].append(\"High care avoidance risk - review cost estimation accuracy\")\n",
    "                if unexpected_cost_rate > 0.3:\n",
    "                    insights_report['recommendations'].append(\"High unexpected cost risk - ensure conservative cost estimates\")\n",
    "\n",
    "# Save insights report\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H%M\")\n",
    "insights_file = f'../artifacts/insights_report_{timestamp}.json'\n",
    "\n",
    "with open(insights_file, 'w') as f:\n",
    "    json.dump(insights_report, f, indent=2)\n",
    "\n",
    "print(f\"ğŸ“Š Analysis Complete!\")\n",
    "print(f\"\\nğŸ’¡ Key Insights:\")\n",
    "for insight in insights_report['insights']:\n",
    "    print(f\"   â€¢ {insight}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Recommendations:\")\n",
    "for rec in insights_report['recommendations']:\n",
    "    print(f\"   â€¢ {rec}\")\n",
    "\n",
    "print(f\"\\nğŸ“ Full insights report saved to: {insights_file}\")\n",
    "print(f\"ğŸ‰ Analysis notebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}