# PRD 001 — RAG Evaluation Harness (Lean Stack: Notebooks + FAISS + Local Files)

## Problem & Goals

### Problem Statement
AI engineers experimenting with RAG systems need a quantitative evaluation framework that runs locally without cloud dependencies. The evaluation harness must measure how retrieval and prompt changes affect downstream performance using LLM-as-Judge, meta-evaluation, and BEIR-style retrieval metrics—all with local files and FAISS (no Azure/Supabase/FastAPI).

### Goals
1. **Quantitative Evaluation Metrics**: BEIR (recall@k, precision@k, nDCG@k), LLM-as-Judge (correctness, hallucination, risk_direction, risk_impact), Meta-Evaluator (judge reliability).
2. **Hallucination & Risk Assessment**: Correctness + hallucination nodes; risk_direction (-1 / 0 / +1), risk_impact (0–3); failure modes (copay, deductible, etc.).
3. **Lean Stack**: Notebook-driven workflow; data/, artifacts/, runs/ on disk; no Azure/Supabase/FastAPI.
4. **Reproducibility**: Deterministic chunking and indexing; runs under `runs/<timestamp>_<name>/` with config.yaml, outputs.jsonl, metrics.json, traces.jsonl.
5. **Modularity**: src/io.py, index.py, eval.py, chunking; prompts loaded from `prompts/evaluation/`.

## User Stories & Acceptance Criteria

### US1: Evaluate RAG Output Quality
- [ ] Invoke LLM-as-Judge with query, retrieved context, model answer, reference answer.
- [ ] Judge returns correctness_binary, hallucination_binary, risk_direction, risk_impact, reasoning, failure_mode.
- [ ] Judge uses prompts loaded from disk when `prompts_dir` is set.

### US2: Validate Judge Reliability
- [ ] Meta-Evaluator validates judge verdicts; returns judge_correct and explanation.
- [ ] Outputs written to `runs/<run_id>/outputs.jsonl` and `metrics.json`.

### US3: Measure Retrieval Performance
- [ ] BEIR metrics (recall@k, precision@k, nDCG@k) computed per example; stored in run outputs.

### US4: Run Full Pipeline from Notebooks
- [ ] 00_setup, 01_ingest_and_index, 02_retrieval_eval, 03_agent_eval, 04_analysis run in sequence.
- [ ] data/ holds corpus.parquet, tasks.jsonl, agent_tasks.jsonl; artifacts/ holds embeddings.npy, faiss.index, docstore.parquet.

## Functional Requirements

### FR1: LLM-as-Judge
- Deterministic orchestration: correctness → hallucination → (if correct) risk_direction + risk_impact.
- Prompts loaded from `prompts/evaluation/` (e.g. correctness.md, hallucination.md, risk_direction.md, risk_impact.md, cost_extraction_prompt.md) when provided to pipeline.

### FR2: Meta-Evaluator
- Deterministic validation of judge outputs; no LLM calls.

### FR3: BEIR Metrics
- Pure Python; recall@k, precision@k, nDCG@k from retrieved chunks and ground_truth_chunk_ids.

### FR4: Storage
- io.py: read/write .npy, .parquet, .jsonl; RunManager for run dirs (config.yaml, outputs.jsonl, metrics.json, traces.jsonl).
- index.py: embeddings + FAISS; docstore.parquet for chunk_id → text.

### FR5: Chunking
- chunking.py outputs chunks; pipeline writes docstore to artifacts/docstore.parquet.

## Non-Functional Requirements

- **Stack**: Python, FAISS, local files only; no Azure/Supabase/FastAPI.
- **Simplicity**: Clear boundaries (io, index, eval, chunking); prompts on disk.
- **Reproducibility**: Versioned runs with full config and traces.

## Out of Scope
- Dashboard/UI; production deployment; Azure/Supabase; FastAPI/workers.

---
**Status**: Draft | **Source**: eval_system PRD001, adapted for raglab lean stack
