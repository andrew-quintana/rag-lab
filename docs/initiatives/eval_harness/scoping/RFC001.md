# RFC 001 — RAG Evaluation Harness Architecture

## Summary
Lean, notebook-driven RAG eval stack: local files + FAISS (no Azure/Supabase/FastAPI). Retrieval = FAISS + docstore.parquet. Evaluation = BEIR + LLM-as-Judge + Meta-Eval; outputs to `runs/<run_id>/`.

## Components
- **io.py**: Read/write .npy, .parquet, .jsonl; run dirs (config, outputs, metrics, traces).
- **index.py**: Embeddings + FAISS; optional Azure embedding wrapper; build/load index and docstore.
- **chunking.py**: Fixed-size chunking; output to docstore.parquet (via pipeline).
- **eval.py**: Load prompts from `prompts/evaluation/`; run BEIR + judge + meta-eval; write to `runs/<run_id>/`.
- **Interfaces**: Query, Chunk, RetrievalResult, JudgeEvaluationResult, MetaEvaluationResult, BEIRMetricsResult (from rag_evaluator).

## Data Flow
1. **Ingest**: corpus → chunking → docstore.parquet; embed → embeddings.npy; FAISS build → faiss.index.
2. **Retrieval eval**: tasks.jsonl + FAISS/docstore → BEIR metrics.
3. **Agent eval**: retriever + generator + judge + meta-eval → outputs.jsonl, metrics.json, traces.jsonl in run dir.

## Constraints
- No Supabase/Azure/FastAPI/workers/queue_client.
- Prompts loaded from disk in eval.py (JudgeEvaluator loads from prompts_dir).
- Run directory layout: config.yaml, outputs.jsonl, metrics.json, traces.jsonl.
