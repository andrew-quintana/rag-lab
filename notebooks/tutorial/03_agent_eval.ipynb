{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Agent Evaluation (LLM-as-Judge + Meta-Eval)\n",
    "\n",
    "This notebook evaluates the complete RAG agent using LLM-as-Judge and meta-evaluation.\n",
    "\n",
    "## Objectives\n",
    "- Set up RAG agent with retrieval + generation\n",
    "- Run complete evaluation pipeline (BEIR + Judge + Meta-eval)\n",
    "- Analyze judge performance and reliability\n",
    "- Generate comprehensive evaluation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport sys\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport logging\nfrom datetime import datetime\n\n# Add src to path\nsys.path.append('../src')\n\n# Import raglab modules\nfrom core.io import DataLoader\nfrom indexing.index import RAGRetriever, EmbeddingProvider\nfrom eval import RAGEvaluationPipeline, run_evaluation, print_evaluation_summary\nfrom core.interfaces import EvaluationExample\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nprint(\"‚úÖ Imports successful\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EMBEDDING_DIM = 768\n",
    "RETRIEVAL_K = 5\n",
    "TEMPERATURE = 0.1  # Low temperature for reproducible evaluation\n",
    "MAX_TOKENS = 500\n",
    "\n",
    "print(f\"üìã Agent Evaluation Configuration:\")\n",
    "print(f\"   Retrieval K: {RETRIEVAL_K}\")\n",
    "print(f\"   LLM Temperature: {TEMPERATURE}\")\n",
    "print(f\"   Max Tokens: {MAX_TOKENS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your LLM and embedding functions\n",
    "def your_llm_function(prompt: str, temperature: float = 0.1, max_tokens: int = 500) -> str:\n",
    "    \"\"\"\n",
    "    Replace this with your actual LLM API call.\n",
    "    This function is used for LLM-as-Judge evaluation.\n",
    "    \n",
    "    Examples:\n",
    "    - OpenAI: openai.ChatCompletion.create(...)\n",
    "    - Azure OpenAI: azure_openai.ChatCompletion.create(...)\n",
    "    - Anthropic: anthropic.messages.create(...)\n",
    "    \"\"\"\n",
    "    # Mock implementation\n",
    "    return f\"Mock LLM response to prompt (temp={temperature}, max_tokens={max_tokens})\"\n",
    "\n",
    "def your_embedding_function(texts: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Replace this with the same embedding function used in previous notebooks.\n",
    "    \"\"\"\n",
    "    return np.random.random((len(texts), EMBEDDING_DIM))\n",
    "\n",
    "def your_generator_function(query: str, context_chunks: list) -> str:\n",
    "    \"\"\"\n",
    "    Replace this with your RAG generation function.\n",
    "    \n",
    "    Args:\n",
    "        query: User question\n",
    "        context_chunks: List of retrieved text chunks\n",
    "        \n",
    "    Returns:\n",
    "        Generated answer based on query and context\n",
    "    \"\"\"\n",
    "    # Mock implementation\n",
    "    context_preview = \" \".join(context_chunks)[:100] if context_chunks else \"No context\"\n",
    "    return f\"Mock answer to '{query[:30]}...' based on context: {context_preview}...\"\n",
    "\n",
    "print(\"‚úÖ Function placeholders defined\")\n",
    "print(\"‚ö†Ô∏è  Remember to replace mock functions with real implementations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Create Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation examples\n",
    "loader = DataLoader(base_path='..')\n",
    "tasks = loader.load_tasks('data/tasks.jsonl')\n",
    "\n",
    "evaluation_examples = []\n",
    "for task in tasks:\n",
    "    example = EvaluationExample(\n",
    "        example_id=task['example_id'],\n",
    "        question=task['question'],\n",
    "        reference_answer=task['reference_answer'],\n",
    "        ground_truth_chunk_ids=task['ground_truth_chunk_ids'],\n",
    "        beir_failure_scale_factor=task.get('beir_failure_scale_factor', 1.0)\n",
    "    )\n",
    "    evaluation_examples.append(example)\n",
    "\n",
    "print(f\"üìö Loaded {len(evaluation_examples)} evaluation examples\")\n",
    "\n",
    "# Display sample examples\n",
    "print(\"\\nüìñ Sample evaluation examples:\")\n",
    "for example in evaluation_examples[:2]:\n",
    "    print(f\"  {example.example_id}: {example.question}\")\n",
    "    print(f\"    Reference: {example.reference_answer[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever\n",
    "embedding_provider = EmbeddingProvider(your_embedding_function)\n",
    "\n",
    "retriever = RAGRetriever(\n",
    "    embedding_provider=embedding_provider,\n",
    "    docstore_path='../artifacts/docstore.parquet',\n",
    "    index_path='../artifacts/faiss.index'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Created retriever with pre-built index\")\n",
    "\n",
    "# Test retrieval\n",
    "test_query = evaluation_examples[0].question\n",
    "test_results = retriever.retrieve(test_query, k=3)\n",
    "print(f\"üîç Test retrieval for '{test_query[:40]}...': {len(test_results)} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test RAG Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the complete RAG pipeline\n",
    "test_example = evaluation_examples[0]\n",
    "print(f\"üß™ Testing RAG pipeline with example: {test_example.example_id}\")\n",
    "print(f\"Query: {test_example.question}\")\n",
    "\n",
    "# Retrieve relevant chunks\n",
    "retrieved_chunks = retriever.retrieve(test_example.question, k=RETRIEVAL_K)\n",
    "context_texts = [chunk.chunk_text for chunk in retrieved_chunks]\n",
    "\n",
    "print(f\"\\nüì• Retrieved {len(retrieved_chunks)} chunks:\")\n",
    "for i, chunk in enumerate(retrieved_chunks[:3]):\n",
    "    print(f\"  {i+1}. {chunk.chunk_id}: {chunk.chunk_text[:60]}...\")\n",
    "\n",
    "# Generate answer\n",
    "generated_answer = your_generator_function(test_example.question, context_texts)\n",
    "print(f\"\\nü§ñ Generated answer: {generated_answer}\")\n",
    "\n",
    "print(f\"\\nüìù Reference answer: {test_example.reference_answer}\")\n",
    "\n",
    "print(\"\\n‚úÖ RAG pipeline test complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Complete Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation run name with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H%M\")\n",
    "run_name = f\"{timestamp}_agent_evaluation\"\n",
    "\n",
    "print(f\"üöÄ Starting complete evaluation pipeline...\")\n",
    "print(f\"   Run name: {run_name}\")\n",
    "print(f\"   Examples: {len(evaluation_examples)}\")\n",
    "print(f\"   This may take several minutes...\")\n",
    "\n",
    "# Run evaluation\n",
    "results, run_dir = run_evaluation(\n",
    "    examples=evaluation_examples,\n",
    "    retriever=retriever,\n",
    "    generator_function=your_generator_function,\n",
    "    llm_function=your_llm_function,\n",
    "    run_name=run_name,\n",
    "    retrieval_k=RETRIEVAL_K\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluation complete!\")\n",
    "print(f\"üìÅ Results saved to: {run_dir}\")\n",
    "print(f\"üìä Evaluated {len(results)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print evaluation summary\n",
    "print_evaluation_summary(run_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis\n",
    "from eval import load_evaluation_results\n",
    "\n",
    "outputs, metrics = load_evaluation_results(run_dir)\n",
    "\n",
    "print(f\"üìä Detailed Results Analysis:\")\n",
    "print(f\"\\nüéØ Overall Performance:\")\n",
    "print(f\"   Total Examples: {metrics['total_examples']}\")\n",
    "print(f\"   Valid Examples: {metrics['valid_examples']}\")\n",
    "print(f\"   Error Rate: {metrics['error_rate']:.2%}\")\n",
    "\n",
    "# Convert outputs to DataFrame for analysis\n",
    "results_df = pd.DataFrame(outputs)\n",
    "\n",
    "# Extract judge verdicts\n",
    "judge_data = []\n",
    "for output in outputs:\n",
    "    judge_output = output['judge_output']\n",
    "    meta_output = output['meta_eval_output']\n",
    "    beir_output = output['beir_metrics']\n",
    "    \n",
    "    judge_data.append({\n",
    "        'example_id': output['example_id'],\n",
    "        'correctness': judge_output['correctness_binary'],\n",
    "        'hallucination': judge_output['hallucination_binary'],\n",
    "        'risk_direction': judge_output.get('risk_direction'),\n",
    "        'risk_impact': judge_output.get('risk_impact'),\n",
    "        'judge_correct': meta_output['judge_correct'],\n",
    "        'recall_at_k': beir_output['recall_at_k'],\n",
    "        'precision_at_k': beir_output['precision_at_k'],\n",
    "        'ndcg_at_k': beir_output['ndcg_at_k']\n",
    "    })\n",
    "\n",
    "judge_df = pd.DataFrame(judge_data)\n",
    "print(f\"\\nüìã Judge Analysis DataFrame created with {len(judge_df)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-example detailed results\n",
    "print(\"üìã Per-Example Results:\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for _, row in judge_df.iterrows():\n",
    "    print(f\"\\n{row['example_id']}:\")\n",
    "    print(f\"  Correctness: {'‚úÖ' if row['correctness'] else '‚ùå'}\")\n",
    "    print(f\"  Hallucination: {'‚ö†Ô∏è' if row['hallucination'] else '‚úÖ'}\")\n",
    "    print(f\"  Judge Correct: {'‚úÖ' if row['judge_correct'] else '‚ùå'}\")\n",
    "    print(f\"  BEIR Metrics: R={row['recall_at_k']:.3f}, P={row['precision_at_k']:.3f}, nDCG={row['ndcg_at_k']:.3f}\")\n",
    "    \n",
    "    if row['risk_direction'] is not None:\n",
    "        risk_desc = {\n",
    "            -1: \"Care Avoidance Risk\",\n",
    "            0: \"No Clear Direction\", \n",
    "            1: \"Unexpected Cost Risk\"\n",
    "        }.get(row['risk_direction'], \"Unknown\")\n",
    "        print(f\"  Risk: {risk_desc} (impact: {row['risk_impact']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive evaluation dashboard\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Judge Performance Distribution\n",
    "judge_performance = [\n",
    "    ('Correct', sum(judge_df['correctness'])),\n",
    "    ('Incorrect', len(judge_df) - sum(judge_df['correctness']))\n",
    "]\n",
    "labels, values = zip(*judge_performance)\n",
    "axes[0,0].pie(values, labels=labels, autopct='%1.1f%%', colors=['lightgreen', 'lightcoral'])\n",
    "axes[0,0].set_title('Judge Correctness Distribution')\n",
    "\n",
    "# 2. Hallucination Detection\n",
    "hallucination_data = [\n",
    "    ('No Hallucination', len(judge_df) - sum(judge_df['hallucination'])),\n",
    "    ('Hallucination Detected', sum(judge_df['hallucination']))\n",
    "]\n",
    "labels, values = zip(*hallucination_data)\n",
    "axes[0,1].pie(values, labels=labels, autopct='%1.1f%%', colors=['lightblue', 'orange'])\n",
    "axes[0,1].set_title('Hallucination Detection')\n",
    "\n",
    "# 3. Meta-Evaluator Performance\n",
    "meta_performance = [\n",
    "    ('Judge Correct', sum(judge_df['judge_correct'])),\n",
    "    ('Judge Incorrect', len(judge_df) - sum(judge_df['judge_correct']))\n",
    "]\n",
    "labels, values = zip(*meta_performance)\n",
    "axes[0,2].pie(values, labels=labels, autopct='%1.1f%%', colors=['mediumseagreen', 'tomato'])\n",
    "axes[0,2].set_title('Meta-Evaluator Assessment')\n",
    "\n",
    "# 4. BEIR Metrics Distribution\n",
    "beir_metrics = ['recall_at_k', 'precision_at_k', 'ndcg_at_k']\n",
    "beir_values = [judge_df[metric].mean() for metric in beir_metrics]\n",
    "metric_names = ['Recall@K', 'Precision@K', 'nDCG@K']\n",
    "bars = axes[1,0].bar(metric_names, beir_values, color=['skyblue', 'lightgreen', 'coral'])\n",
    "axes[1,0].set_title('Average BEIR Metrics')\n",
    "axes[1,0].set_ylim(0, 1.0)\n",
    "axes[1,0].set_ylabel('Score')\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, beir_values):\n",
    "    axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                  f'{value:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 5. Risk Direction Distribution (if available)\n",
    "risk_directions = judge_df['risk_direction'].dropna()\n",
    "if len(risk_directions) > 0:\n",
    "    risk_counts = risk_directions.value_counts().sort_index()\n",
    "    risk_labels = {-1: 'Care\\nAvoidance', 0: 'No Clear\\nDirection', 1: 'Unexpected\\nCost'}\n",
    "    labels = [risk_labels.get(idx, f'Risk {idx}') for idx in risk_counts.index]\n",
    "    axes[1,1].bar(labels, risk_counts.values, color=['lightcoral', 'lightgray', 'orange'])\n",
    "    axes[1,1].set_title('Risk Direction Distribution')\n",
    "    axes[1,1].set_ylabel('Count')\n",
    "else:\n",
    "    axes[1,1].text(0.5, 0.5, 'No Risk Data\\nAvailable', ha='center', va='center', transform=axes[1,1].transAxes)\n",
    "    axes[1,1].set_title('Risk Direction Distribution')\n",
    "\n",
    "# 6. Correlation: Retrieval Quality vs Judge Performance\n",
    "axes[1,2].scatter(judge_df['recall_at_k'], judge_df['correctness'], alpha=0.7, color='purple')\n",
    "axes[1,2].set_xlabel('Recall@K')\n",
    "axes[1,2].set_ylabel('Correctness (0/1)')\n",
    "axes[1,2].set_title('Retrieval vs Judge Performance')\n",
    "axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(f'RAG Agent Evaluation Dashboard - {run_name}', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example-level performance heatmap\n",
    "performance_cols = ['correctness', 'hallucination', 'judge_correct', 'recall_at_k', 'precision_at_k', 'ndcg_at_k']\n",
    "performance_data = judge_df[['example_id'] + performance_cols].set_index('example_id')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(performance_data.T, annot=True, cmap='RdYlGn', cbar_kws={'label': 'Score'}, fmt='.3f')\n",
    "plt.title('Per-Example Performance Heatmap', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Metrics')\n",
    "plt.xlabel('Example ID')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate insights and recommendations\n",
    "print(\"üîç Evaluation Insights and Recommendations:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Overall Performance\n",
    "correctness_rate = judge_df['correctness'].mean()\n",
    "hallucination_rate = judge_df['hallucination'].mean()\n",
    "judge_accuracy = judge_df['judge_correct'].mean()\n",
    "avg_recall = judge_df['recall_at_k'].mean()\n",
    "\n",
    "print(f\"\\nüìä Key Performance Indicators:\")\n",
    "print(f\"   Correctness Rate: {correctness_rate:.1%}\")\n",
    "print(f\"   Hallucination Rate: {hallucination_rate:.1%}\")\n",
    "print(f\"   Judge Accuracy: {judge_accuracy:.1%}\")\n",
    "print(f\"   Average Recall@{RETRIEVAL_K}: {avg_recall:.3f}\")\n",
    "\n",
    "# 2. Performance Analysis\n",
    "print(f\"\\nüéØ Performance Analysis:\")\n",
    "if correctness_rate >= 0.8:\n",
    "    print(\"   ‚úÖ Strong correctness performance (‚â•80%)\")\n",
    "elif correctness_rate >= 0.6:\n",
    "    print(\"   ‚ö†Ô∏è  Moderate correctness performance (60-80%)\")\n",
    "else:\n",
    "    print(\"   ‚ùå Low correctness performance (<60%) - needs improvement\")\n",
    "\n",
    "if hallucination_rate <= 0.2:\n",
    "    print(\"   ‚úÖ Low hallucination rate (‚â§20%)\")\n",
    "elif hallucination_rate <= 0.4:\n",
    "    print(\"   ‚ö†Ô∏è  Moderate hallucination rate (20-40%)\")\n",
    "else:\n",
    "    print(\"   ‚ùå High hallucination rate (>40%) - requires attention\")\n",
    "\n",
    "if judge_accuracy >= 0.8:\n",
    "    print(\"   ‚úÖ High judge reliability (‚â•80%)\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Judge reliability could be improved (<80%)\")\n",
    "\n",
    "# 3. Recommendations\n",
    "print(f\"\\nüí° Recommendations:\")\n",
    "\n",
    "if avg_recall < 0.7:\n",
    "    print(\"   üîÑ Consider improving retrieval:\")\n",
    "    print(\"     - Try different embedding models\")\n",
    "    print(\"     - Adjust chunk size and overlap\")\n",
    "    print(\"     - Experiment with query expansion\")\n",
    "\n",
    "if hallucination_rate > 0.3:\n",
    "    print(\"   üõ°Ô∏è  Reduce hallucinations:\")\n",
    "    print(\"     - Improve prompt engineering\")\n",
    "    print(\"     - Add stricter grounding instructions\")\n",
    "    print(\"     - Implement confidence scoring\")\n",
    "\n",
    "if correctness_rate < 0.7:\n",
    "    print(\"   üìà Improve answer quality:\")\n",
    "    print(\"     - Enhance generation prompts\")\n",
    "    print(\"     - Increase retrieval context\")\n",
    "    print(\"     - Consider few-shot examples\")\n",
    "\n",
    "print(f\"\\nüìÅ Full evaluation details available in: {run_dir}\")\n",
    "print(f\"üéâ Agent evaluation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}