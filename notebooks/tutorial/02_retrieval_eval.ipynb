{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Retrieval Evaluation (BEIR Metrics)\n",
    "\n",
    "This notebook evaluates the retrieval system using BEIR-style metrics.\n",
    "\n",
    "## Objectives\n",
    "- Load evaluation tasks with ground truth chunk IDs\n",
    "- Run retrieval for each query\n",
    "- Compute BEIR metrics (Recall@K, Precision@K, nDCG@K)\n",
    "- Analyze retrieval performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport sys\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport logging\n\n# Add src to path\nsys.path.append('../src')\n\n# Import raglab modules\nfrom core.io import DataLoader, RunManager\nfrom indexing.index import RAGRetriever, EmbeddingProvider\nfrom beir_metrics import compute_beir_metrics\nfrom core.interfaces import EvaluationExample\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nprint(\"‚úÖ Imports successful\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "K_VALUES = [1, 3, 5, 10]  # Different k values for evaluation\n",
    "EMBEDDING_DIM = 768\n",
    "\n",
    "# Your embedding function (should match the one used in indexing)\n",
    "def your_embedding_function(texts: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Replace this with the same embedding function used in notebook 01.\n",
    "    \"\"\"\n",
    "    return np.random.random((len(texts), EMBEDDING_DIM))\n",
    "\n",
    "print(f\"üìã Evaluation configuration:\")\n",
    "print(f\"   K values: {K_VALUES}\")\n",
    "print(f\"   Embedding dimension: {EMBEDDING_DIM}\")\n",
    "print(\"‚ö†Ô∏è  Ensure embedding function matches the one used for indexing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation tasks\n",
    "loader = DataLoader(base_path='..')\n",
    "tasks = loader.load_tasks('data/tasks.jsonl')\n",
    "\n",
    "print(f\"üìö Loaded {len(tasks)} evaluation tasks\")\n",
    "\n",
    "# Convert to EvaluationExample objects\n",
    "evaluation_examples = []\n",
    "for task in tasks:\n",
    "    example = EvaluationExample(\n",
    "        example_id=task['example_id'],\n",
    "        question=task['question'],\n",
    "        reference_answer=task['reference_answer'],\n",
    "        ground_truth_chunk_ids=task['ground_truth_chunk_ids'],\n",
    "        beir_failure_scale_factor=task.get('beir_failure_scale_factor', 1.0)\n",
    "    )\n",
    "    evaluation_examples.append(example)\n",
    "\n",
    "print(f\"‚úÖ Created {len(evaluation_examples)} evaluation examples\")\n",
    "\n",
    "# Display sample tasks\n",
    "print(\"\\nüìñ Sample evaluation tasks:\")\n",
    "for example in evaluation_examples[:3]:\n",
    "    print(f\"  {example.example_id}: {example.question}\")\n",
    "    print(f\"    Ground truth chunks: {example.ground_truth_chunk_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load retriever\n",
    "embedding_provider = EmbeddingProvider(your_embedding_function)\n",
    "\n",
    "retriever = RAGRetriever(\n",
    "    embedding_provider=embedding_provider,\n",
    "    docstore_path='../artifacts/docstore.parquet',\n",
    "    index_path='../artifacts/faiss.index'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Loaded retriever with pre-built index\")\n",
    "\n",
    "# Test retriever\n",
    "test_results = retriever.retrieve(evaluation_examples[0].question, k=3)\n",
    "print(f\"üîç Test query returned {len(test_results)} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run retrieval evaluation for different k values\n",
    "retrieval_results = []\n",
    "\n",
    "for example in evaluation_examples:\n",
    "    print(f\"üîç Evaluating: {example.example_id}\")\n",
    "    \n",
    "    # Retrieve for maximum k value\n",
    "    max_k = max(K_VALUES)\n",
    "    retrieved_chunks = retriever.retrieve(example.question, k=max_k)\n",
    "    \n",
    "    # Compute metrics for each k value\n",
    "    example_results = {\n",
    "        'example_id': example.example_id,\n",
    "        'question': example.question,\n",
    "        'ground_truth_chunk_ids': example.ground_truth_chunk_ids,\n",
    "        'retrieved_chunk_ids': [chunk.chunk_id for chunk in retrieved_chunks]\n",
    "    }\n",
    "    \n",
    "    for k in K_VALUES:\n",
    "        # Compute BEIR metrics for this k\n",
    "        metrics = compute_beir_metrics(\n",
    "            retrieved_chunks[:k],\n",
    "            example.ground_truth_chunk_ids,\n",
    "            k=k\n",
    "        )\n",
    "        \n",
    "        example_results[f'recall_at_{k}'] = metrics.recall_at_k\n",
    "        example_results[f'precision_at_{k}'] = metrics.precision_at_k\n",
    "        example_results[f'ndcg_at_{k}'] = metrics.ndcg_at_k\n",
    "    \n",
    "    retrieval_results.append(example_results)\n",
    "\n",
    "print(f\"\\n‚úÖ Completed retrieval evaluation for {len(evaluation_examples)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "results_df = pd.DataFrame(retrieval_results)\n",
    "\n",
    "print(\"üìä Retrieval Results:\")\n",
    "print(results_df.head())\n",
    "\n",
    "# Compute average metrics across all examples\n",
    "print(\"\\nüìà Average BEIR Metrics:\")\n",
    "\n",
    "for k in K_VALUES:\n",
    "    recall_col = f'recall_at_{k}'\n",
    "    precision_col = f'precision_at_{k}'\n",
    "    ndcg_col = f'ndcg_at_{k}'\n",
    "    \n",
    "    avg_recall = results_df[recall_col].mean()\n",
    "    avg_precision = results_df[precision_col].mean()\n",
    "    avg_ndcg = results_df[ndcg_col].mean()\n",
    "    \n",
    "    print(f\"\\n  K={k}:\")\n",
    "    print(f\"    Recall@{k}:    {avg_recall:.3f}\")\n",
    "    print(f\"    Precision@{k}: {avg_precision:.3f}\")\n",
    "    print(f\"    nDCG@{k}:      {avg_ndcg:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed per-example analysis\n",
    "print(\"üìã Per-Example Results:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"\\n{row['example_id']}: {row['question'][:50]}...\")\n",
    "    print(f\"  Ground truth: {row['ground_truth_chunk_ids']}\")\n",
    "    print(f\"  Retrieved: {row['retrieved_chunk_ids'][:5]}\")\n",
    "    \n",
    "    # Show metrics for k=5\n",
    "    print(f\"  Metrics@5: R={row['recall_at_5']:.3f}, P={row['precision_at_5']:.3f}, nDCG={row['ndcg_at_5']:.3f}\")\n",
    "    \n",
    "    # Check if ground truth chunks are in retrieved results\n",
    "    retrieved_set = set(row['retrieved_chunk_ids'][:5])\n",
    "    gt_set = set(row['ground_truth_chunk_ids'])\n",
    "    found = gt_set.intersection(retrieved_set)\n",
    "    \n",
    "    if found:\n",
    "        print(f\"  ‚úÖ Found ground truth chunks: {list(found)}\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå No ground truth chunks found in top-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of metrics by K value\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "metrics = ['recall', 'precision', 'ndcg']\n",
    "colors = ['blue', 'green', 'red']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    k_vals = []\n",
    "    metric_vals = []\n",
    "    \n",
    "    for k in K_VALUES:\n",
    "        k_vals.append(k)\n",
    "        col_name = f'{metric}_at_{k}'\n",
    "        metric_vals.append(results_df[col_name].mean())\n",
    "    \n",
    "    axes[i].plot(k_vals, metric_vals, marker='o', color=colors[i], linewidth=2, markersize=8)\n",
    "    axes[i].set_title(f'{metric.capitalize()}@K', fontsize=14, fontweight='bold')\n",
    "    axes[i].set_xlabel('K')\n",
    "    axes[i].set_ylabel(f'{metric.capitalize()}')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].set_ylim(0, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('BEIR Retrieval Metrics by K Value', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of metrics per example\n",
    "metric_cols = []\n",
    "for k in K_VALUES:\n",
    "    metric_cols.extend([f'recall_at_{k}', f'precision_at_{k}', f'ndcg_at_{k}'])\n",
    "\n",
    "heatmap_data = results_df[['example_id'] + metric_cols].set_index('example_id')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_data.T, annot=True, cmap='RdYlGn', vmin=0, vmax=1, \n",
    "            cbar_kws={'label': 'Metric Value'}, fmt='.3f')\n",
    "plt.title('Retrieval Metrics Heatmap by Example', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Metric@K')\n",
    "plt.xlabel('Example ID')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "run_manager = RunManager()\n",
    "run_dir = run_manager.create_run_dir('retrieval_evaluation')\n",
    "\n",
    "# Save configuration\n",
    "config = {\n",
    "    'evaluation_type': 'retrieval_beir_metrics',\n",
    "    'k_values': K_VALUES,\n",
    "    'num_examples': len(evaluation_examples),\n",
    "    'embedding_dim': EMBEDDING_DIM\n",
    "}\n",
    "run_manager.save_config(config, run_dir)\n",
    "\n",
    "# Save detailed results\n",
    "run_manager.save_outputs(retrieval_results, run_dir)\n",
    "\n",
    "# Compute and save summary metrics\n",
    "summary_metrics = {}\n",
    "for k in K_VALUES:\n",
    "    summary_metrics[f'avg_recall_at_{k}'] = results_df[f'recall_at_{k}'].mean()\n",
    "    summary_metrics[f'avg_precision_at_{k}'] = results_df[f'precision_at_{k}'].mean()\n",
    "    summary_metrics[f'avg_ndcg_at_{k}'] = results_df[f'ndcg_at_{k}'].mean()\n",
    "\n",
    "run_manager.save_metrics(summary_metrics, run_dir)\n",
    "\n",
    "print(f\"‚úÖ Saved retrieval evaluation results to: {run_dir}\")\n",
    "print(f\"üìÅ Run directory contains:\")\n",
    "for file in Path(run_dir).iterdir():\n",
    "    print(f\"   {file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Retrieval Evaluation Summary:\")\n",
    "print(f\"   Examples evaluated: {len(evaluation_examples)}\")\n",
    "print(f\"   K values tested: {K_VALUES}\")\n",
    "\n",
    "# Best performing K\n",
    "best_recall_k = max(K_VALUES, key=lambda k: results_df[f'recall_at_{k}'].mean())\n",
    "best_ndcg_k = max(K_VALUES, key=lambda k: results_df[f'ndcg_at_{k}'].mean())\n",
    "\n",
    "print(f\"\\nüìä Best Performance:\")\n",
    "print(f\"   Best Recall@K: K={best_recall_k} ({results_df[f'recall_at_{best_recall_k}'].mean():.3f})\")\n",
    "print(f\"   Best nDCG@K: K={best_ndcg_k} ({results_df[f'ndcg_at_{best_ndcg_k}'].mean():.3f})\")\n",
    "\n",
    "# Identify problematic examples\n",
    "poor_examples = results_df[results_df['recall_at_5'] < 0.5]\n",
    "if len(poor_examples) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  Examples with low recall@5 (< 0.5): {len(poor_examples)}\")\n",
    "    for _, row in poor_examples.iterrows():\n",
    "        print(f\"     {row['example_id']}: recall={row['recall_at_5']:.3f}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All examples have recall@5 ‚â• 0.5\")\n",
    "\n",
    "print(f\"\\nüéâ Ready for notebook 03_agent_eval.ipynb!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}