{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Ingest and Index Documents\n",
    "\n",
    "This notebook handles document chunking, embedding generation, and FAISS index creation.\n",
    "\n",
    "## Objectives\n",
    "- Load and chunk documents\n",
    "- Generate embeddings for all chunks\n",
    "- Build FAISS index for similarity search\n",
    "- Create docstore mapping for chunk retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport sys\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport logging\n\n# Add src to path\nsys.path.append('../src')\n\n# Import raglab modules\nfrom core.io import DataLoader\nfrom chunking import chunk_text\nfrom indexing.index import EmbeddingProvider, LocalFAISSIndex, RAGRetriever\nfrom core.interfaces import Chunk\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nprint(\"âœ… Imports successful\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up chunking and embedding parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "CHUNK_SIZE = 500  # Characters per chunk\n",
    "CHUNK_OVERLAP = 100  # Character overlap between chunks\n",
    "EMBEDDING_DIM = 768  # Dimension of embeddings (adjust based on your model)\n",
    "\n",
    "# Your embedding function from setup notebook\n",
    "def your_embedding_function(texts: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Replace this with your actual embedding function.\n",
    "    This should return embeddings with shape (len(texts), EMBEDDING_DIM)\n",
    "    \"\"\"\n",
    "    # Mock implementation - replace with real embeddings\n",
    "    return np.random.random((len(texts), EMBEDDING_DIM))\n",
    "\n",
    "print(f\"ğŸ“‹ Configuration:\")\n",
    "print(f\"   Chunk size: {CHUNK_SIZE} characters\")\n",
    "print(f\"   Chunk overlap: {CHUNK_OVERLAP} characters\")\n",
    "print(f\"   Embedding dimension: {EMBEDDING_DIM}\")\n",
    "print(\"âš ï¸  Remember to replace the mock embedding function with a real one\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Chunk Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus\n",
    "loader = DataLoader(base_path='..')\n",
    "corpus_df = loader.load_corpus('data/corpus.parquet')\n",
    "\n",
    "print(f\"ğŸ“š Loaded corpus with {len(corpus_df)} documents\")\n",
    "print(\"\\nğŸ“– Sample documents:\")\n",
    "for idx, row in corpus_df.head(3).iterrows():\n",
    "    print(f\"  {row['doc_id']}: {row['text'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk all documents\n",
    "all_chunks = []\n",
    "\n",
    "for idx, row in corpus_df.iterrows():\n",
    "    doc_id = row['doc_id']\n",
    "    text = row['text']\n",
    "    \n",
    "    # Chunk the document\n",
    "    chunks = chunk_text(\n",
    "        text=text,\n",
    "        document_id=doc_id,\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        overlap=CHUNK_OVERLAP\n",
    "    )\n",
    "    \n",
    "    all_chunks.extend(chunks)\n",
    "    print(f\"ğŸ“„ {doc_id}: {len(chunks)} chunks\")\n",
    "\n",
    "print(f\"\\nâœ… Total chunks created: {len(all_chunks)}\")\n",
    "\n",
    "# Display sample chunks\n",
    "print(\"\\nğŸ“Š Sample chunks:\")\n",
    "for i, chunk in enumerate(all_chunks[:3]):\n",
    "    print(f\"  {chunk.chunk_id}: {chunk.text[:60]}...\")\n",
    "    print(f\"    Document: {chunk.document_id}, Length: {len(chunk.text)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding provider\n",
    "embedding_provider = EmbeddingProvider(your_embedding_function)\n",
    "\n",
    "# Extract chunk texts for embedding\n",
    "chunk_texts = [chunk.text for chunk in all_chunks]\n",
    "chunk_ids = [chunk.chunk_id for chunk in all_chunks]\n",
    "\n",
    "print(f\"ğŸ”„ Generating embeddings for {len(chunk_texts)} chunks...\")\n",
    "print(\"   This may take a moment depending on your embedding provider\")\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = embedding_provider.embed_texts(chunk_texts)\n",
    "\n",
    "print(f\"âœ… Generated embeddings with shape: {embeddings.shape}\")\n",
    "\n",
    "# Save embeddings\n",
    "loader.save_embeddings(embeddings, 'artifacts/embeddings.npy')\n",
    "print(\"ğŸ’¾ Saved embeddings to artifacts/embeddings.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS index\n",
    "faiss_index = LocalFAISSIndex(\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    index_type=\"flat\"  # Use \"ivf\" or \"hnsw\" for larger datasets\n",
    ")\n",
    "\n",
    "# Add embeddings to index\n",
    "faiss_index.add_embeddings(embeddings, chunk_ids)\n",
    "\n",
    "print(f\"âœ… Built FAISS index with {len(chunk_ids)} vectors\")\n",
    "\n",
    "# Save index\n",
    "faiss_index.save('artifacts/faiss.index')\n",
    "print(\"ğŸ’¾ Saved FAISS index to artifacts/faiss.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Docstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create docstore mapping chunk_id -> text/metadata\n",
    "docstore_data = []\n",
    "\n",
    "for chunk in all_chunks:\n",
    "    docstore_data.append({\n",
    "        'chunk_id': chunk.chunk_id,\n",
    "        'text': chunk.text,\n",
    "        'document_id': chunk.document_id,\n",
    "        'metadata': chunk.metadata or {}\n",
    "    })\n",
    "\n",
    "docstore_df = pd.DataFrame(docstore_data)\n",
    "\n",
    "# Save docstore\n",
    "loader.save_docstore(docstore_df, 'artifacts/docstore.parquet')\n",
    "\n",
    "print(f\"âœ… Created docstore with {len(docstore_df)} entries\")\n",
    "print(\"ğŸ’¾ Saved docstore to artifacts/docstore.parquet\")\n",
    "\n",
    "# Display sample docstore entries\n",
    "print(\"\\nğŸ“Š Sample docstore entries:\")\n",
    "print(docstore_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever and test it\n",
    "retriever = RAGRetriever(\n",
    "    embedding_provider=embedding_provider,\n",
    "    docstore_path='../artifacts/docstore.parquet',\n",
    "    index_path='../artifacts/faiss.index'\n",
    ")\n",
    "\n",
    "# Test query\n",
    "test_query = \"What is a copayment?\"\n",
    "results = retriever.retrieve(test_query, k=3)\n",
    "\n",
    "print(f\"ğŸ” Test query: '{test_query}'\")\n",
    "print(f\"ğŸ“Š Retrieved {len(results)} results:\")\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\n  {i+1}. Chunk {result.chunk_id} (score: {result.similarity_score:.3f})\")\n",
    "    print(f\"     Text: {result.chunk_text[:100]}...\")\n",
    "\n",
    "print(\"\\nâœ… Retrieval test successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Index creation completed successfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"ğŸ“ˆ Indexing Summary:\")\n",
    "print(f\"   Documents processed: {len(corpus_df)}\")\n",
    "print(f\"   Total chunks created: {len(all_chunks)}\")\n",
    "print(f\"   Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"   FAISS index size: {len(chunk_ids)} vectors\")\n",
    "\n",
    "print(\"\\nğŸ“ Generated artifacts:\")\n",
    "artifacts_dir = Path('../artifacts')\n",
    "if artifacts_dir.exists():\n",
    "    for file in artifacts_dir.iterdir():\n",
    "        if file.is_file():\n",
    "            size_mb = file.stat().st_size / (1024 * 1024)\n",
    "            print(f\"   {file.name}: {size_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\nğŸ‰ Ready for notebook 02_retrieval_eval.ipynb!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}